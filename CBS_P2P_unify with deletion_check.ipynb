{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycelonis import get_celonis\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://bbraun.eu-3.celonis.cloud/'\n",
    "api_token = 'Y2Q5NWY1ZTctZDk2YS00YWNlLWIwNWEtOGVmYTdiYzg3YTA4OnZKcGxjaEluTUtkSmIvbXZZdk40YnczdFU3UUJKTXJ5MllpQ2xLdGhCRHFJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 16:48:58 - pycelonis: Login successful! The Application Key currently has access to 688 Analyses and to 4 Data Pools.\n"
     ]
    }
   ],
   "source": [
    "celonis = get_celonis()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Pool, id d1b99aa8-5ded-4410-b2bc-9659f9d53ba6, name SAP ERP>,\n",
       "<Pool, id e596e1ae-1cec-4dd5-90ea-98b8ae6b99d7, name SAP ERP DEC>,\n",
       "<Pool, id 2a0c6eba-87d9-4205-9d86-b726f3292659, name MS SQL Celonis>,\n",
       "<Pool, id 0778113c-cd5f-4502-9ba3-ba7c65649a36, name OpenText - Vendor Invoice Management>,]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "celonis.pools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = celonis.pools.find('d1b99aa8-5ded-4410-b2bc-9659f9d53ba6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<DataConnection, id 507a7454-3f8b-4598-a904-36886f516085, name USB>,\n",
       "<DataConnection, id 43200d69-cf72-4943-a9a0-e849c7c3b1bd, name MYB>,\n",
       "<DataConnection, id 8a3688b3-78f2-4ccf-a00f-6ad01ecfd99c, name BRB>,\n",
       "<DataConnection, id 26da4b43-22f6-428f-b1e1-f25234904108, name USB-Test>,\n",
       "<DataConnection, id d52c7f88-d09d-406e-930a-bf94a09cdb78, name DEB>,\n",
       "<DataConnection, id a52ba2b3-224d-4fb9-bcb1-e25dff382bde, name DEB - test>,\n",
       "<DataConnection, id b38be222-b287-48e9-a743-f799ed8ae560, name DEA>,\n",
       "<DataConnection, id 272a6a8b-f885-427f-a20f-4f2bcbc0aa62, name USA>,\n",
       "<DataConnection, id 5299a972-b4e0-42f8-bb3d-d293210291d8, name MYA>,\n",
       "<DataConnection, id 366e11f3-c369-4b0e-bc4d-2cba64ee21ec, name BRA>,]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool.data_connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pool.tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CBS_P2P_EKKO',\n",
       " 'CBS_P2P_NAME_MAPPING_COLUMNS',\n",
       " 'CBS_TMP_P2P_EKPO_EBAN',\n",
       " 'CBS_P2P_LFA1',\n",
       " 'CBS_P2P_BSEG',\n",
       " 'CBS_P2P_LFM1',\n",
       " 'CBS_TMP_TO',\n",
       " 'CBS_P2P_RBKP',\n",
       " 'CBS_TMP_P2P_BKPF_BSEG',\n",
       " 'CBS_TMP_P2P_EKKO_EKPO',\n",
       " 'CBS_P2P_MARC',\n",
       " 'CBS_TMP_P2P_CASES_EKKO',\n",
       " 'CBS_P2P_EINE',\n",
       " 'CBS__CEL_P2P_CASES',\n",
       " 'CBS_P2P_BUKRS_REGION_MAPPING',\n",
       " 'CBS_P2P_TO',\n",
       " 'CBS_P2P_MARA',\n",
       " 'CBS__CEL_P2P_ACTIVITIES',\n",
       " 'CBS_P2P_EKES',\n",
       " 'CBS_P2P_EKPO',\n",
       " 'CBS__CEL_P2P_LINKED_BSEG',\n",
       " 'CBS__CEL_P2P_ACTIVITY_MASTER_DATA',\n",
       " 'CBS_P2P_QALS',\n",
       " 'CBS_P2P_RSEG',\n",
       " 'CBS_P2P_EBAN',\n",
       " 'CBS__CEL_P2P_CASES_EKKO',\n",
       " 'CBS_P2P_T001',\n",
       " 'CBS_P2P_QPCD',\n",
       " 'CBS_P2P_QAVE',\n",
       " 'CBS_P2P_BKPF',\n",
       " 'CBS_P2P_MSEG',\n",
       " 'CBS_P2P_NAME_MAPPING_TABLES',\n",
       " 'CBS_P2P_QPGR',\n",
       " 'CBS_P2P_LFB1',\n",
       " 'CBS_P2P_EKET',\n",
       " 'CBS_P2P_CONTRACT_INFO',\n",
       " 'CBS_P2P_QPAM',\n",
       " 'CBS_P2P_EKBE']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(\n",
    "df.query('name.str.contains(\"CBS_\")', engine='python').name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os.path import dirname, abspath\n",
    "#_PARENT_DIR = dirname(dirname(dirname(abspath(__file__))))\n",
    "#sys.path.append(_PARENT_DIR)\n",
    "#from utils._utils import parse_celonis_url, get_user_data, get_logger              ### <--\n",
    "from pycelonis import get_celonis\n",
    "import argparse\n",
    "#logger = get_logger()\n",
    "def unify_tables(url, api_token, datapool, tables, tableSchema, function = 'null'):\n",
    "    \n",
    "    #loop through defined tables\n",
    "    #for table_name in tables:\n",
    "       \n",
    "    c = get_celonis(url, api_token)\n",
    "    #get_user_data(c, 'Table Unifier')\n",
    "    #url_options= parse_celonis_url(url)                                            ### <--\n",
    "    #pool = c.pools.find(url_options['id'])                                         ### <--\n",
    "    \n",
    "    pool = c.pools.find(datapool)                                                   ### <--\n",
    "\n",
    "    minReq = []\n",
    "    maxReq = []\n",
    "    sameTables = []\n",
    "    schemalist = tableSchema                                                        # added to extract required table schemas (in python list format)\n",
    "    columns = ''\n",
    "    select = ''\n",
    "    \n",
    "    # create or find global data job \n",
    "    try:\n",
    "        global_dj = pool.data_jobs.find('TEST123')\n",
    "    except:\n",
    "        global_dj = pool.create_data_job(name = 'TEST123')\n",
    "    \n",
    "    #loop through defined tables\n",
    "    for table_name in tables:\n",
    "    \n",
    "        # find tables to unify\n",
    "        x = 0\n",
    "        for schema in global_dj.tables:\n",
    "            for table in schema:\n",
    "                x = x + 1\n",
    "                if table['schemaName'] not in schemalist:\n",
    "                    continue\n",
    "                if table['name'] == table_name:\n",
    "                    sameTables.append(table)\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        if len(sameTables) == 0:\n",
    "            print('TABLE', table_name, 'CAN NOT BE UNIFIED! Not existing in source systems!')\n",
    "         \n",
    "            try: \n",
    "                global_dj.transformations.find(\"Unify \" + table_name).delete()\n",
    "                global_dj.create_transformation(name = \"Unify \" + table_name, statement = '-- no existing tables to unify in defined source systems')\n",
    "                continue\n",
    "            except:\n",
    "                global_dj.create_transformation(name = \"Unify \" + table_name, statement = '-- no existing tables to unify in defined source systems')\n",
    "                continue\n",
    "        else:\n",
    "            # create minimal column requirement\n",
    "            for col in sameTables[0]['columns']:\n",
    "                try:\n",
    "                    if col in sameTables[1]['columns']:                                              # ERROR CHECKING !\n",
    "                        minReq.append(col)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            \"\"\"\n",
    "            #only required if function = 'min'\n",
    "\n",
    "            if len(sameTables) > 2:\n",
    "                for table in sameTables[2:]:\n",
    "                    for col in minReq:\n",
    "                        if col not in table['columns']:\n",
    "                            minReq.pop(col)\n",
    "                # create transformation statement for minimal table requirement\n",
    "\n",
    "            if function == 'min':\n",
    "                for col in minReq:\n",
    "                    if(minReq.index(col) == 0):\n",
    "                        columns += '{}\\n'.format(col['name'])\n",
    "                    else:\n",
    "                        columns += ',{}\\n'.format(col['name'])\n",
    "                for table in sameTables:\n",
    "                    select += 'SELECT \\n{}FROM \"{}\".\"{}\"'.format(columns, table['schemaName'], table_name)\n",
    "                    if table != sameTables[len(sameTables)-1]:\n",
    "                        select += '\\nUNION ALL\\n'\n",
    "            \"\"\"            \n",
    "\n",
    "            # create transformation for statement for extension with null\n",
    "            if function == 'null':\n",
    "                #columns = ''\n",
    "                #select = ''\n",
    "                \n",
    "                for table in sameTables:\n",
    "                    for col in table['columns']:\n",
    "                        if col not in maxReq:\n",
    "                            maxReq.append(col)\n",
    "                for table in sameTables:\n",
    "                    for col in maxReq:\n",
    "                        if(maxReq.index(col) == 0):\n",
    "                            columns += \"'\" + table['schemaName'] + \"' \" + 'AS \"SCHEMA\"\\n' + ',{}\\n'.format('\"' + col['name'] + '\"' if col in table['columns'] else 'NULL AS \"' + col['name'] + '\"')\n",
    "                        else:\n",
    "                            columns += ',{}\\n'.format('\"' + col['name'] + '\"' if col in table['columns'] else 'NULL AS \"' + col['name'] + '\"')\n",
    "                    select += 'SELECT \\n{}FROM \"{}\".\"{}\"'.format(columns, table['schemaName'], table_name)\n",
    "                    columns = ''\n",
    "                    if table != sameTables[len(sameTables)-1]:\n",
    "                        select += '\\nUNION ALL\\n'\n",
    "            # create transformation\n",
    "            #statement = 'DROP VIEW IF EXISTS \"{}_UNIFIED\";\\n\\nCreate VIEW \"{}_UNIFIED\" AS(\\n{}\\n);'.format(table_name, table_name, select)\n",
    "            statement = 'DROP VIEW IF EXISTS \"{}\";\\n\\nCreate VIEW \"{}\" AS(\\n{}\\n);'.format(table_name, table_name, select)                      ### <-- Unified\n",
    "            try: \n",
    "                    global_dj.transformations.find(\"Unify \" + table_name).delete()\n",
    "            except: \n",
    "                pass\n",
    "            \n",
    "            global_dj.create_transformation(name = \"Unify \" + table_name, statement = statement)\n",
    "            print('The union statement has been saved in the global data job \"Unify Tables\" for Table', table_name)\n",
    "            select = ''\n",
    "            sameTables = []\n",
    "            minReq = []\n",
    "            maxReq = []\n",
    "            \n",
    "            # reminder: datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-04 15:03:08 - pycelonis: Login successful! Hello Application Key, this key currently has access to 688 analyses.\n",
      "2021-02-04 15:03:18 - pycelonis: Best matches: [(0.98, 'Unify CBSO_O2C_KNB1'), (0.96, 'Unify CBSO_O2C_NAST'), (0.96, 'Unify CBSO_O2C_KNKK')]\n",
      "The union statement has been saved in the global data job \"Unify Tables\" for Table CBSO_O2C_KNA1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# URL to the Cloud Data Pool where the tables to unify are hosted.\n",
    "url = 'https://bbraun.eu-3.celonis.cloud/integration/ui/pools/d1b99aa8-5ded-4410-b2bc-9659f9d53ba6'\n",
    "\n",
    "# Specify a valid API token for your Cloud Team.\n",
    "api_token = 'MmNjYzZiMzctZDhhOS00ZDRlLTkxZWMtYzg2NDM5NjUzN2E4OkgrSExiaWtiQWsvbWRkOU0zTEltVXlPMmdGUkZLajROa2diYytBVlZXa3lu'\n",
    "\n",
    "# Specify a datapool \n",
    "datapool = 'd1b99aa8-5ded-4410-b2bc-9659f9d53ba6'\n",
    "\n",
    "# Specify the table names for which the tables should be merged. \n",
    "#tables = ['VBEH']\n",
    "\n",
    "tables = ['CBSO_O2C_KNA1'\n",
    " ]\n",
    "\n",
    "# Specify 'min' if the unified table should be the minimal column requirement and 'null' if the not existing columns should be filled with null values.\n",
    "function = 'null'\n",
    "\n",
    "# Specify the required table schemas in a python list\n",
    "tableSchema = ['d1b99aa8-5ded-4410-b2bc-9659f9d53ba6_272a6a8b-f885-427f-a20f-4f2bcbc0aa62',\n",
    "               'd1b99aa8-5ded-4410-b2bc-9659f9d53ba6_366e11f3-c369-4b0e-bc4d-2cba64ee21ec',\n",
    "               'd1b99aa8-5ded-4410-b2bc-9659f9d53ba6_5299a972-b4e0-42f8-bb3d-d293210291d8',\n",
    "               'd1b99aa8-5ded-4410-b2bc-9659f9d53ba6_b38be222-b287-48e9-a743-f799ed8ae560']\n",
    "unify_tables(url = url, api_token = api_token, datapool = datapool, tables = tables, tableSchema = tableSchema, function = function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
